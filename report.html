<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>Machine Learning Engineer Nanodegree</title>
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
        
        <style>
.task-list-item { list-style-type: none; } .task-list-item-checkbox { margin-left: -20px; vertical-align: middle; }
</style>
        <style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        
        
    </head>
    <body class="vscode-light">
        <h1 id="machine-learning-engineer-nanodegree">Machine Learning Engineer Nanodegree</h1>
<h2 id="capstone-project">Capstone Project</h2>
<p>Jinesh Mehta<br>
November 25th, 2019</p>
<h2 id="i-definition">I. <strong>Definition</strong></h2>
<h3 id="project-overview">Project Overview</h3>
<p>A lot of work has been done in the field of image recognition over the past few years. In this
project a specific object recognition problem was examined. The goal is to build a model capable of
identifying to which breed a dog belongs, based only on its photo. To train and evaluate the model
the custom dataset provided by udacity, containing 13233 total human images and 8351 total dog images,
was used.</p>
<h3 id="problem-statement">Problem Statement</h3>
<p>There are several factors that make the problem of dogs categorization challenging. Firstly, there
many different breeds of dogs exist, and all breeds on a high level look alike, i.e. there can be only
subtle differences in appearance between some breeds. In fact, a dog’s breed might be not obvious
right away even for people who have an expertise in the domain. Therefore, determination of dog
breeds provides an excellent domain for fine grained visual categorization experiments. Secondly,
dog images are very rich in their variety, showing dogs of all shapes, sizes, and colors, under
differing illumination, in innumerable poses, and in just about any location. The photos have different
resolutions, backgrounds, and scales. On some images the dogs are are partially covered by other
objects or wear clothes such as hats, scarfs, glasses. Thus, there is a lot of noise on many of the photos
that makes the problem more challenging. Thirdly, our data set is small in terms of the number
of photos per breed.</p>
<p>If humans want to distinguish a dog from a cat, or from some other object, then they need to look
at things like ears, whiskers, tails, tongues, fur textures, and so forth. These are the features
that we (humans) use to discriminate. But none of these features are available to a computer, which receive
only a matrix of independent integers as an input. Models such as Convolutional Neural Networks (CNN)
allow computers to automatically extract hierarchies of features from raw pixels. These techniques
proved to be successful in a variety of visual analysis tasks. Using these techniques, we can create a dog-breed
classifier and allay that to solve our problem.</p>
<p>The complete solution consists of the following steps:</p>
<ul>
<li><strong>Import Datasets</strong> : Our first step will be to load the datasets that are divided into train, validation and test folders.</li>
<li><strong>Detect Humans</strong> : In this section, we use OpenCV's implementation of Haar feature-based cascade classifiers to detect human faces in images.</li>
<li><strong>Detect Dogs</strong> : In this section, we use a pre-trained VGG-16 model to detect dogs in images.</li>
<li><strong>Create and train a CNN to classify Dog Breeds (from scratch)</strong> : In this step, we will create a CNN-based model from scratch that classifies dog breeds. In the next step, we will use an approach based on transfer learning.</li>
<li><strong>Train a CNN to Classify Dog Breeds (via transfer learning)</strong> : We will now use transfer learning to create a CNN that can identify dog breed from images. We will reuse the data loaders that we created earlier.</li>
<li><strong>Dog breed classification algorithm</strong> :
<ul>
<li>if a dog is detected in the image, return the predicted breed.</li>
<li>if a human is detected in the image, return the resembling dog breed.</li>
<li>if neither is detected in the image, provide output as other thing.</li>
</ul>
</li>
<li><strong>Testing the algorithm</strong> : Lastly, We will test our algorithm on six sample images to measure the performance of our algorithm.</li>
</ul>
<h3 id="metrics">Metrics</h3>
<p>For this particular project, we are only going to focus on an accuracy score.
The goal of what we’re looking to do here is pretty simple: we want to see how well we can do at classifying breeds of dogs.
Accuracy will be able to tell us in a simple and easy-to-understand way how well our deep classification model is performing in this regard.</p>
<h2 id="ii-analysis">II. <strong>Analysis</strong></h2>
<h3 id="data-exploration">Data Exploration</h3>
<p>Following are the required human and dog datasets:</p>
<ul>
<li>To download the <a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/dogImages.zip">dog dataset</a></li>
<li>To download the <a href="https://s3-us-west-1.amazonaws.com/udacity-aind/dog-project/lfw.zip">human dataset</a></li>
</ul>
<p><em>Note: If you are using a Windows machine, you are encouraged to use 7zip to extract the folder.</em></p>
<p>Above custom dataset is provided by Udacity. It contains 13233 total human images and 8351 total dog images.</p>
<h3 id="algorithms-and-techniques">Algorithms and Techniques</h3>
<p>In this section, we discuss the algorithms and techniques used for solving this problem.</p>
<h4 id="convolutional-neural-networks-cnns">Convolutional Neural Networks (CNNs)</h4>
<p>A convolutional neural network is a class of deep, feed-forward artificial neural networks that has
successfully been applied to analyzing visual imagery. CNNs take advantage of the fact that the
input consists of images and they constrain the architecture in a more sensible way. The layers of
a CNN have neurons arranged in 3 dimensions in so called convolution layers. In the convolution
layer, the filters are passed across the input, row by row, and they activate when they see some type
of visual feature. Now, rather than treating each of the raw pixel values in isolation, CNN’s treat
them in small local groups. These sliding filters are how the CNN can learn meaningful features and
locate them in any part of the image.</p>
<p><img src="file:///c:\Users\H242848\Documents\Udacity\capstone\Flow.png" alt="alt text" title="Example of CNN classification for a user input image"></p>
<p>These is only a high level overview of the CNN. There are also many other details related to other
layers (pooling, relu, fully-connected, dropout), loss functions ( cross-entropy, MSE, MAE, cosine
similarity), optimizers (SGD, RMV, Adagrad), etc.</p>
<p>There is almost a linear relationship in the amount of data required and the size of the model. General
idea is that the model should be large enough to capture relations in the data along with specifics
of our problem. Early layers of the model capture high level relations between the different parts
of the input (like edges and patterns), whereas later layers capture information that usually helps to
discriminate between the desired outputs. Therefore if the complexity of the problem is high (like
Image Classification) the number of parameters and the amount of data required is also large.</p>
<h3 id="benchmark">Benchmark</h3>
<p><img src="file:///c:\Users\H242848\Documents\Udacity\capstone\table.png" alt="alt text" title="Comparsion of different existing models"></p>
<h2 id="iii-methodology">III. <strong>Methodology</strong></h2>
<h3 id="data-preprocessing">Data Preprocessing</h3>
<p>Before we can push our data through our algorithm, we’ll have to pre-process it so that it appropriately conforms to the format we will need it in.
Because we are using a TensorFlow-backed Keras CNN, this means that we will need to tranform the data appropriately.
The images are resized by doing a crop of random size of the original size and a random aspect ratio of the original aspect ratio.
This crop is finally resized to 224x224 pixels in order to be able to reuse the same data loaders for the transfer learning step.</p>
<h3 id="implementation">Implementation</h3>
<h4 id="architecture">Architecture</h4>
<pre><code class="language-python"><div><span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn
<span class="hljs-keyword">import</span> torch.nn.functional <span class="hljs-keyword">as</span> F

<span class="hljs-comment"># define the CNN architecture</span>
<span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Net</span><span class="hljs-params">(nn.Module)</span>:</span>
    <span class="hljs-comment">### <span class="hljs-doctag">TODO:</span> choose an architecture, and complete the class</span>
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">__init__</span><span class="hljs-params">(self)</span>:</span>
        super(Net, self).__init__()
        <span class="hljs-comment">## Define layers of a CNN</span>
        self.features = nn.Sequential(
            <span class="hljs-comment"># 1st 2D convolution layer</span>
            nn.Conv2d(<span class="hljs-number">3</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),
            nn.ReLU(inplace=<span class="hljs-literal">True</span>),
            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),
            
            <span class="hljs-comment"># Defining another 2D convolution layer</span>
            nn.Conv2d(<span class="hljs-number">16</span>, <span class="hljs-number">32</span>, kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),
            nn.ReLU(inplace=<span class="hljs-literal">True</span>),
            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),
            
            <span class="hljs-comment"># Defining another 2D convolution layer</span>
            nn.Conv2d(<span class="hljs-number">32</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),
            nn.ReLU(inplace=<span class="hljs-literal">True</span>),
            nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),
            
            nn.Dropout(p=<span class="hljs-number">0.5</span>),
            nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>)
        )
        
        self.classifier = nn.Sequential(
            nn.Linear(<span class="hljs-number">64</span> * <span class="hljs-number">14</span> * <span class="hljs-number">14</span>, <span class="hljs-number">133</span>),
            nn.LogSoftmax(dim=<span class="hljs-number">1</span>)
        )
    
    <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">forward</span><span class="hljs-params">(self, x)</span>:</span>
        <span class="hljs-comment">## Define forward behavior</span>
        out = self.features(x)
        out = out.view(<span class="hljs-number">-1</span>, <span class="hljs-number">64</span>*<span class="hljs-number">14</span>*<span class="hljs-number">14</span>)
        out = self.classifier(out)
        
        <span class="hljs-keyword">return</span> out
    

<span class="hljs-comment"># instantiate the CNN</span>
model_scratch = Net()

<span class="hljs-comment"># move tensors to GPU if CUDA is available</span>
<span class="hljs-keyword">if</span> use_cuda:
    model_scratch.cuda()
</div></code></pre>
<p>The architecture is composed from a feature extractor and a classifier.
The feature extractor is has 3 CNN layers in to extract features.
Each CNN layer has a ReLU activation and a 2D max pooling layer in to reduce the amount of parameters and computation in the network.
After the CNN layers we have a dropout layer with a probability of 0.5 in to prevent overfitting and an average pooling layer to calculate the average for each patch of the feature map.
The classifier is a fully connected layer with an input shape of 64 x 14 x 14 (which matches the output from the average pooling layer) and 133 nodes, one for each class (there are 133 dog breeds).
We add a softmax activation to get the probabilities for each class.</p>
<h4 id="training-and-validation">Training and Validation</h4>
<p>For training and validating our model, we execute the following code snippet below :</p>
<pre><code class="language-python"><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">train</span><span class="hljs-params">(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path)</span>:</span>
    <span class="hljs-string">"""returns trained model"""</span>
    <span class="hljs-comment"># initialize tracker for minimum validation loss</span>
    valid_loss_min = np.Inf 
    
    <span class="hljs-keyword">for</span> epoch <span class="hljs-keyword">in</span> range(<span class="hljs-number">1</span>, n_epochs+<span class="hljs-number">1</span>):
        <span class="hljs-comment"># initialize variables to monitor training and validation loss</span>
        train_loss = <span class="hljs-number">0.0</span>
        valid_loss = <span class="hljs-number">0.0</span>
        
        <span class="hljs-comment">###################</span>
        <span class="hljs-comment"># train the model #</span>
        <span class="hljs-comment">###################</span>
        model.train()
        <span class="hljs-keyword">for</span> batch_idx, (data, target) <span class="hljs-keyword">in</span> enumerate(loaders[<span class="hljs-string">'train'</span>]):
            <span class="hljs-comment"># move to GPU</span>
            <span class="hljs-keyword">if</span> use_cuda:
                data, target = data.cuda(), target.cuda()
            <span class="hljs-comment">## find the loss and update the model parameters accordingly</span>
            <span class="hljs-comment">## record the average training loss, using something like</span>
            <span class="hljs-comment">## train_loss = train_loss + ((1 / (batch_idx + 1)) * (loss.data - train_loss))</span>
            
            <span class="hljs-comment">#Set the parameter gradients to zero</span>
            optimizer.zero_grad()
            
            <span class="hljs-comment">#Forward pass, backward pass, optimize</span>
            outputs = model(data)
            loss = criterion(outputs, target)
            loss.backward()
            optimizer.step()
            
            train_loss += ((<span class="hljs-number">1</span> / (batch_idx + <span class="hljs-number">1</span>)) * (loss.data - train_loss))
            
        <span class="hljs-comment">######################    </span>
        <span class="hljs-comment"># validate the model #</span>
        <span class="hljs-comment">######################</span>
        model.eval()
        <span class="hljs-keyword">for</span> batch_idx, (data, target) <span class="hljs-keyword">in</span> enumerate(loaders[<span class="hljs-string">'valid'</span>]):
            <span class="hljs-comment"># move to GPU</span>
            <span class="hljs-keyword">if</span> use_cuda:
                data, target = data.cuda(), target.cuda()
            <span class="hljs-comment">## update the average validation loss</span>
            val_outputs = model(data)
            val_loss = criterion(val_outputs, target)
            valid_loss += ((<span class="hljs-number">1</span> / (batch_idx + <span class="hljs-number">1</span>)) * (val_loss.data - valid_loss))

            
        <span class="hljs-comment"># print training/validation statistics </span>
        print(<span class="hljs-string">'Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'</span>.format(
            epoch, 
            train_loss,
            valid_loss
            ))
        
        <span class="hljs-comment">## <span class="hljs-doctag">TODO:</span> save the model if validation loss has decreased</span>
        <span class="hljs-keyword">if</span>(valid_loss &lt; valid_loss_min):
            print(<span class="hljs-string">'Saving model: Validation Loss: {:.6f} decreased \tOld Validation Loss: {:.6f}'</span>.format(valid_loss, valid_loss_min))
            valid_loss_min = valid_loss
            torch.save(model.state_dict(), save_path)
            
    <span class="hljs-comment"># return trained model</span>
    <span class="hljs-keyword">return</span> model


<span class="hljs-comment"># train the model</span>
model_scratch = train(<span class="hljs-number">50</span>, loaders_scratch, model_scratch, optimizer_scratch, criterion_scratch, use_cuda, <span class="hljs-string">'model_scratch.pt'</span>)

<span class="hljs-comment"># load the model that got the best validation accuracy</span>
model_scratch.load_state_dict(torch.load(<span class="hljs-string">'model_scratch.pt'</span>))
</div></code></pre>
<p>Above Snippet resulted in the following output for 50 Epochs:</p>
<pre><code><div>Epoch: 1 	Training Loss: 4.804974 	Validation Loss: 4.615381
Saving model: Validation Loss: 4.615381 decreased 	Old Validation Loss: inf
Epoch: 2 	Training Loss: 4.602738 	Validation Loss: 4.448377
Saving model: Validation Loss: 4.448377 decreased 	Old Validation Loss: 4.615381
Epoch: 3 	Training Loss: 4.491667 	Validation Loss: 4.389207
Saving model: Validation Loss: 4.389207 decreased 	Old Validation Loss: 4.448377
Epoch: 4 	Training Loss: 4.400146 	Validation Loss: 4.396991
Epoch: 5 	Training Loss: 4.352615 	Validation Loss: 4.280646
Saving model: Validation Loss: 4.280646 decreased 	Old Validation Loss: 4.389207
Epoch: 6 	Training Loss: 4.306862 	Validation Loss: 4.252617
Saving model: Validation Loss: 4.252617 decreased 	Old Validation Loss: 4.280646
Epoch: 7 	Training Loss: 4.269252 	Validation Loss: 4.190484
Saving model: Validation Loss: 4.190484 decreased 	Old Validation Loss: 4.252617
Epoch: 8 	Training Loss: 4.211070 	Validation Loss: 4.275696
Epoch: 9 	Training Loss: 4.174200 	Validation Loss: 4.127232
Saving model: Validation Loss: 4.127232 decreased 	Old Validation Loss: 4.190484
Epoch: 10 	Training Loss: 4.168478 	Validation Loss: 4.148942
Epoch: 11 	Training Loss: 4.141559 	Validation Loss: 4.127430
Epoch: 12 	Training Loss: 4.104400 	Validation Loss: 4.143802
Epoch: 13 	Training Loss: 4.087573 	Validation Loss: 4.095686
Saving model: Validation Loss: 4.095686 decreased 	Old Validation Loss: 4.127232
Epoch: 14 	Training Loss: 4.057649 	Validation Loss: 4.024379
Saving model: Validation Loss: 4.024379 decreased 	Old Validation Loss: 4.095686
Epoch: 15 	Training Loss: 4.028504 	Validation Loss: 4.043189
Epoch: 16 	Training Loss: 3.991086 	Validation Loss: 3.963461
Saving model: Validation Loss: 3.963461 decreased 	Old Validation Loss: 4.024379
Epoch: 17 	Training Loss: 3.983787 	Validation Loss: 4.086076
Epoch: 18 	Training Loss: 3.954470 	Validation Loss: 4.004037
Epoch: 19 	Training Loss: 3.912849 	Validation Loss: 3.975426
Epoch: 20 	Training Loss: 3.898360 	Validation Loss: 4.016487
Epoch: 21 	Training Loss: 3.867416 	Validation Loss: 4.000227
Epoch: 22 	Training Loss: 3.868152 	Validation Loss: 3.974565
Epoch: 23 	Training Loss: 3.823937 	Validation Loss: 3.960703
Saving model: Validation Loss: 3.960703 decreased 	Old Validation Loss: 3.963461
Epoch: 24 	Training Loss: 3.837263 	Validation Loss: 3.966237
Epoch: 25 	Training Loss: 3.807810 	Validation Loss: 3.894585
Saving model: Validation Loss: 3.894585 decreased 	Old Validation Loss: 3.960703
Epoch: 26 	Training Loss: 3.792684 	Validation Loss: 3.880291
Saving model: Validation Loss: 3.880291 decreased 	Old Validation Loss: 3.894585
Epoch: 27 	Training Loss: 3.792550 	Validation Loss: 3.912843
Epoch: 28 	Training Loss: 3.755440 	Validation Loss: 3.997811
Epoch: 29 	Training Loss: 3.753458 	Validation Loss: 3.870937
Saving model: Validation Loss: 3.870937 decreased 	Old Validation Loss: 3.880291
Epoch: 30 	Training Loss: 3.726488 	Validation Loss: 3.869637
Saving model: Validation Loss: 3.869637 decreased 	Old Validation Loss: 3.870937
Epoch: 31 	Training Loss: 3.712027 	Validation Loss: 3.860836
Saving model: Validation Loss: 3.860836 decreased 	Old Validation Loss: 3.869637
Epoch: 32 	Training Loss: 3.728802 	Validation Loss: 3.894773
Epoch: 33 	Training Loss: 3.674767 	Validation Loss: 3.964670
Epoch: 34 	Training Loss: 3.669846 	Validation Loss: 3.895880
Epoch: 35 	Training Loss: 3.638517 	Validation Loss: 3.862057
Epoch: 36 	Training Loss: 3.658729 	Validation Loss: 3.877197
Epoch: 37 	Training Loss: 3.623693 	Validation Loss: 4.010448
Epoch: 38 	Training Loss: 3.600378 	Validation Loss: 3.906092
Epoch: 39 	Training Loss: 3.628024 	Validation Loss: 3.942724
Epoch: 40 	Training Loss: 3.582008 	Validation Loss: 3.789247
Saving model: Validation Loss: 3.789247 decreased 	Old Validation Loss: 3.860836
Epoch: 41 	Training Loss: 3.595245 	Validation Loss: 3.810999
Epoch: 42 	Training Loss: 3.592308 	Validation Loss: 3.847783
Epoch: 43 	Training Loss: 3.580878 	Validation Loss: 3.771963
Saving model: Validation Loss: 3.771963 decreased 	Old Validation Loss: 3.789247
Epoch: 44 	Training Loss: 3.567358 	Validation Loss: 3.806099
Epoch: 45 	Training Loss: 3.548939 	Validation Loss: 3.923195
Epoch: 46 	Training Loss: 3.555041 	Validation Loss: 3.810476
Epoch: 47 	Training Loss: 3.537040 	Validation Loss: 3.901146
Epoch: 48 	Training Loss: 3.551056 	Validation Loss: 3.740501
Saving model: Validation Loss: 3.740501 decreased 	Old Validation Loss: 3.771963
Epoch: 49 	Training Loss: 3.520532 	Validation Loss: 3.980445
Epoch: 50 	Training Loss: 3.512998 	Validation Loss: 4.055543
</div></code></pre>
<h4 id="testing">Testing</h4>
<p>Now we try out our model on the test dataset of dog images. Use the code snippet below to calculate the test loss and accuracy.</p>
<pre><code class="language-python"><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">test</span><span class="hljs-params">(loaders, model, criterion, use_cuda)</span>:</span>

    <span class="hljs-comment"># monitor test loss and accuracy</span>
    test_loss = <span class="hljs-number">0.</span>
    correct = <span class="hljs-number">0.</span>
    total = <span class="hljs-number">0.</span>

    model.eval()
    <span class="hljs-keyword">for</span> batch_idx, (data, target) <span class="hljs-keyword">in</span> enumerate(loaders[<span class="hljs-string">'test'</span>]):
        <span class="hljs-comment"># move to GPU</span>
        <span class="hljs-keyword">if</span> use_cuda:
            data, target = data.cuda(), target.cuda()
        <span class="hljs-comment"># forward pass: compute predicted outputs by passing inputs to the model</span>
        output = model(data)
        <span class="hljs-comment"># calculate the loss</span>
        loss = criterion(output, target)
        <span class="hljs-comment"># update average test loss </span>
        test_loss = test_loss + ((<span class="hljs-number">1</span> / (batch_idx + <span class="hljs-number">1</span>)) * (loss.data - test_loss))
        <span class="hljs-comment"># convert output probabilities to predicted class</span>
        pred = output.data.max(<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)[<span class="hljs-number">1</span>]
        <span class="hljs-comment"># compare predictions to true label</span>
        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())
        total += data.size(<span class="hljs-number">0</span>)
            
    print(<span class="hljs-string">'Test Loss: {:.6f}\n'</span>.format(test_loss))

    print(<span class="hljs-string">'\nTest Accuracy: %2d%% (%2d/%2d)'</span> % (
        <span class="hljs-number">1.</span>   * correct / total, correct, total))

<span class="hljs-comment"># call test function    </span>
test(loaders_scratch, model_scratch, criterion_scratch, use_cuda)
</div></code></pre>
<p>Above code provides the following output which indicates a loss of 3.65 and an accuracy of 15% which is very less and
hence a refinement is needed.</p>
<pre><code><div>Test Loss: 3.654766

Test Accuracy: 15% (133/836)
</div></code></pre>
<h3 id="refinement">Refinement</h3>
<p>Since creating a CNN from scratch didn’t do too great, we’ll leverage one of the architectures packaged up nicely for us
from the options between VGG-16, VGG-19, ResNet-50, InceptionV3, and Xception. We use the VGG16 network that was pretrained on the ImageNet dataset.
The ImageNet dataset containes similar images with our own dataset so we can keep the feature extractor part.
Because we have a small dataset and we don't need to identify dogs but dog breeds
we replace the classifier with a fully connected layer with 1024 nodes, with ReLU and a dropout with a 0.4 probability,
connected to an output layer with 133 nodes (same as the number of classes).
We then train the model but we freeze the parameters for the feature extractor so only the classifier paramters get backpropagated.</p>
<h4 id="architecture-1">Architecture</h4>
<p>We use transfer learning to create a CNN to classify dog breed by executing the following code snippet below:</p>
<pre><code class="language-python"><div><span class="hljs-keyword">import</span> torchvision.models <span class="hljs-keyword">as</span> models
<span class="hljs-keyword">import</span> torch.nn <span class="hljs-keyword">as</span> nn

model_transfer = models.vgg16(pretrained=<span class="hljs-literal">True</span>)

<span class="hljs-keyword">for</span> param <span class="hljs-keyword">in</span> model_transfer.parameters():
    param.requires_grad = <span class="hljs-literal">False</span>

classifier = nn.Sequential(
    nn.Linear(<span class="hljs-number">25088</span>, <span class="hljs-number">1024</span>),
    nn.ReLU(inplace=<span class="hljs-literal">True</span>),
    nn.Dropout(p=<span class="hljs-number">0.4</span>),
    nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">133</span>),
    nn.LogSoftmax(dim=<span class="hljs-number">1</span>)
)

model_transfer.classifier = classifier 

<span class="hljs-keyword">if</span> use_cuda:
    model_transfer = model_transfer.cuda()
</div></code></pre>
<h4 id="training-and-validation-1">Training and Validation</h4>
<p>For training and validating our model, we execute the following code snippet below :</p>
<pre><code class="language-python"><div><span class="hljs-comment"># train the model</span>
model_transfer = train(<span class="hljs-number">10</span>, loaders_transfer, model_transfer, optimizer_transfer, criterion_transfer, use_cuda, <span class="hljs-string">'model_transfer.pt'</span>)

<span class="hljs-comment"># load the model that got the best validation accuracy (uncomment the line below)</span>
model_transfer.load_state_dict(torch.load(<span class="hljs-string">'model_transfer.pt'</span>))
</div></code></pre>
<p>Above Snippet resulted in the following output for 10 Epochs:</p>
<pre><code><div>Epoch: 1 	Training Loss: 4.126252 	Validation Loss: 1.948809
Saving model: Validation Loss: 1.948809 decreased 	Old Validation Loss: inf
Epoch: 2 	Training Loss: 2.914006 	Validation Loss: 1.389281
Saving model: Validation Loss: 1.389281 decreased 	Old Validation Loss: 1.948809
Epoch: 3 	Training Loss: 2.662741 	Validation Loss: 1.194920
Saving model: Validation Loss: 1.194920 decreased 	Old Validation Loss: 1.389281
Epoch: 4 	Training Loss: 2.601928 	Validation Loss: 1.202487
Epoch: 5 	Training Loss: 2.523065 	Validation Loss: 1.224847
Epoch: 6 	Training Loss: 2.514232 	Validation Loss: 1.044022
Saving model: Validation Loss: 1.044022 decreased 	Old Validation Loss: 1.194920
Epoch: 7 	Training Loss: 2.434076 	Validation Loss: 1.065839
Epoch: 8 	Training Loss: 2.478743 	Validation Loss: 1.210468
Epoch: 9 	Training Loss: 2.429160 	Validation Loss: 1.097689
Epoch: 10 	Training Loss: 2.413055 	Validation Loss: 1.134307
</div></code></pre>
<h4 id="testing-1">Testing</h4>
<p>Now we again try out our model on the test dataset of dog images.
Use the code snippet below to calculate the test loss and accuracy :</p>
<pre><code class="language-python"><div>test(loaders_transfer, model_transfer, criterion_transfer, use_cuda)
</div></code></pre>
<p>Above code provides the following output which indicates a loss of 1.08 and an accuracy of 66% which is much better than our previous approach.</p>
<pre><code><div>Test Loss: 1.081452

Test Accuracy: 66% (560/836)
</div></code></pre>
<h2 id="ivexperiments-and-results">IV.<strong>Experiments and Results</strong></h2>
<h3 id="model-evaluation-and-validation">Model Evaluation and Validation</h3>
<p>Now that we’ve created a model architecture using transfer learning,
I’ll test out my model on both some dogs as well as other random images to see how it necessarily performed. For that,
I have created  an algorithm that accepts a file path to an image and
first determines whether the image contains a human, dog, or neither.
Then,</p>
<ul>
<li>if a dog is detected in the image, return the predicted breed.</li>
<li>if a human is detected in the image, return the resembling dog breed.</li>
<li>if neither is detected in the image, provide output that indicates an error.</li>
</ul>
<p>The code snippet for the above mentioned process is as follows :</p>
<pre><code class="language-python"><div><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">process_image</span><span class="hljs-params">(image)</span>:</span>
    <span class="hljs-string">''' Scales, crops, and normalizes a PIL image for a PyTorch model,
        returns an Numpy array
    '''</span>
    
    <span class="hljs-comment"># scale</span>
    scale_size = <span class="hljs-number">256</span>,<span class="hljs-number">256</span>
    image.thumbnail(scale_size, Image.LANCZOS)

    <span class="hljs-comment"># crop</span>
    crop_size = <span class="hljs-number">224</span>
    width, height = image.size   <span class="hljs-comment"># Get dimensions</span>
    
    left = (width - crop_size)/<span class="hljs-number">2</span>
    top = (height - crop_size)/<span class="hljs-number">2</span>
    right = (width + crop_size)/<span class="hljs-number">2</span>
    bottom = (height + crop_size)/<span class="hljs-number">2</span>
    
    image = image.crop((left, top, right, bottom))
    
    <span class="hljs-comment"># normalize</span>
    mean = np.array([<span class="hljs-number">0.485</span>, <span class="hljs-number">0.456</span>, <span class="hljs-number">0.406</span>])
    std = np.array([<span class="hljs-number">0.229</span>, <span class="hljs-number">0.224</span>, <span class="hljs-number">0.225</span>])
    image_array = np.array(image) / <span class="hljs-number">255</span>
    
    image = (image_array - mean) / std
    
    <span class="hljs-comment"># reorder dimensions</span>
    image = image.transpose((<span class="hljs-number">2</span>,<span class="hljs-number">0</span>,<span class="hljs-number">1</span>))
    
    <span class="hljs-keyword">return</span> torch.from_numpy(image)

<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">predict_breed_transfer</span><span class="hljs-params">(img_path)</span>:</span>
    <span class="hljs-comment"># load the image and return the predicted breed</span>
        <span class="hljs-comment"># open image</span>
    image = Image.open(img_path)
    image = process_image(image)
    image = image.unsqueeze_(<span class="hljs-number">0</span>)
    <span class="hljs-keyword">if</span> use_cuda:
        image = image.cuda().float()
    
    model_transfer.eval()
    
    <span class="hljs-keyword">with</span> torch.no_grad():
        output = model_transfer(image)
        <span class="hljs-comment"># convert output probabilities to predicted class</span>
        pred = output.data.max(<span class="hljs-number">1</span>, keepdim=<span class="hljs-literal">True</span>)[<span class="hljs-number">1</span>]
    
    <span class="hljs-keyword">return</span> class_names[pred]
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">run_app</span><span class="hljs-params">(img_path)</span>:</span>
    <span class="hljs-comment">## handle cases for a human face, dog, and neither</span>
    is_dog = <span class="hljs-literal">False</span>
    <span class="hljs-keyword">if</span> dog_detector(img_path):
        is_dog = <span class="hljs-literal">True</span>
        title = <span class="hljs-string">'Cute doggie!'</span>
    <span class="hljs-keyword">elif</span> face_detector(img_path):
        title = <span class="hljs-string">"Hello, Budddy!"</span>
    <span class="hljs-keyword">else</span>:
        title = <span class="hljs-string">"Hello, Whatever!"</span>
        
    
    breed = predict_breed_transfer(img_path)
    
    <span class="hljs-keyword">if</span> is_dog:
        sub_title = <span class="hljs-string">f'You are a <span class="hljs-subst">{breed}</span>, aren\'t you?!?'</span>
    <span class="hljs-keyword">else</span>:
        sub_title = <span class="hljs-string">f'You look like a...<span class="hljs-subst">{breed}</span>. Hmm, weird!!!'</span>
        
    image = Image.open(img_path)
    plt.imshow(image, interpolation=<span class="hljs-string">'nearest'</span>)
    plt.axis(<span class="hljs-string">'off'</span>)
    plt.title(<span class="hljs-string">f'<span class="hljs-subst">{title}</span>\n<span class="hljs-subst">{sub_title}</span>'</span>)
    plt.show()
    plt.close()

images = [
    <span class="hljs-string">'images/test/dog_affenpinscher.jpg'</span>,
    <span class="hljs-string">'images/test/Leonardo_Dicaprio.jpg'</span>,
    <span class="hljs-string">'images/test/Basenji.jpg'</span>,
    <span class="hljs-string">'images/test/ocicat.jpg'</span>,
    <span class="hljs-string">'images/test/savannah.jpg'</span>,
    <span class="hljs-string">'images/test/tom_cruise.jpg'</span>]
<span class="hljs-keyword">for</span> file <span class="hljs-keyword">in</span> images:
    run_app(file)
</div></code></pre>
<p>As shown above in the code snippet, we use 6 images (combination of cat, dog and human images) for testing the whole algorithm.</p>
<p><img src="file:///c:\Users\H242848\Documents\Udacity\capstone\results.png" alt="alt text" title="Results"></p>
<p>When we got to testing out human images, that’s when things started to get interesting.
The classifier did correctly predict that every human image I tested out was a human image,
but the type of dog breed it guessed the human to be produced some interesting results.
However, there are certain classification where the algorithm failed things to guess which can be seen in test results.</p>
<h2 id="v-conclusion">V. <strong>Conclusion</strong></h2>
<p>In this work we tried to tackle the dog breed identification problem using a very small dataset with only a few dozens of images per breed.
First, a small convolutional neural network was built and trained on the dataset from scratch, and the accuracy of 15% was achieved on the testing dataset. Then this result was further improved by applying VGG-16 using transfer learning. The testing accuracy thereafter improved to 66%.
This result is very impressive considering the complexity of the problem, and the limited amount of data.</p>
<p>Even though at first glance the dog breed identification seems to be a very challenging problem, it was shown that a powerful and highly accurate CNN-based image classification model can be built with help of transfer learning.</p>
<h3 id="improvement">Improvement</h3>
<p>This project took one relatively simple approach to creating a CNN, but there a number of ways in which we could have improved the effort.
Here are three things we could have altered along the way:</p>
<ul>
<li><strong>More training data</strong>: Relatively speaking, we trained our model on a very low number of images. Most of these highly complex architectures are trained on vastly more images than our training dataset, so additional training data would almost certainly help to reinforce the outcomes we would expect.</li>
<li><strong>Altering the CNN’s architecture</strong>: This one is harder to know, but given that the architectures like VGG or the Inception one used here contained many layers, it’s feasible to think that adding more layers / adjusting the existing layers would produce better outputs.</li>
<li><strong>Adjusting the transfer learning architecture</strong>: For this project, I arbitrarily selected VGG16 as the architecture for our transfer learning. I didn’t try any of the others like Xception, and it could have happened that they would have been a better fit for our algorithm.</li>
</ul>
<h3 id="reflection">Reflection</h3>
<p>I’ve already stated this above, but clearly transfer learning is the way to go if an option.
It helped our model to perform much better, and it was more efficient computationally.
Computational efficiency is starting to emerge as a key factor in environmental friendliness.
Also, another point to consider is the amount of energy it takes to run these deep learning models is astoundingly high.
If we all try creating deep learning models from scratch, we contribute to this problem.
Transfer learning helps to cut down these environmental concerns by sharing knowledge and cutting down energy consumption.</p>
<h3 id="references"><strong>References</strong></h3>
<ul>
<li>For <a href="http://blizzard.cs.uwaterloo.ca/iss4e/wp-content/uploads/2017/10/yerbol_aussat_cs698_project_report.pdf"> Problem Statement Brief </a></li>
<li>For <a href="https://medium.com/datadriveninvestor/using-deep-learning-to-classify-breeds-of-dogs-from-images-2b026ea03436"> Template &amp; Structure </a></li>
</ul>

    </body>
    </html>